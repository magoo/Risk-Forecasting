
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="keywords" content="remark,remarkjs,markdown,slideshow,presentation" />
    <meta name="description" content="A simple, in-browser, markdown-driven slideshow tool." />
    <title>Risk Forecasting</title>
    <link rel="stylesheet" href="./font-awesome/css/font-awesome.min.css">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif);
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body {
        font-family: 'Droid Serif';
      }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .green { color: green}
      .large { font-size: 2em; }
      a, a > code {
        color: #c0c8d3;
        text-decoration: underline;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      em {
        font-style: italic;
        filter: brightness(120%);
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #242424;
        color: #9e9e9e;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #f3f3f3;
        line-height: 1em;
      }


      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
        .left-column h2:last-of-type, .left-column h3:last-child {
          color: #000;
        }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

    name: inverse
    layout: true
    class: center, middle, inverse
    ---

    # Risk for Engineers
    ### Measuring the reduction of "_bad things_" in our future

    ---

    ## Problem:

    --

    ### I build security things, but I can't measure what difference it made.

    ---

    ## Here's a simple way to measure the difference you've made.

    ---

    ## Think of what security "thing" you work on.

    --

    ### Now think of a specific scenario that you're hoping it will prevent.

    ---

    ### Think of that scenario with a specific time frame (Days, Months, Years)

    ---
    name: final
    layout: true
    class: left, top, inverse
    ---

    ## Examples:

    --

    ### Data is stolen from our production storage buckets this quarter.

    --

    ### We discover malware on our CEO's laptop this year.

    --

    ### We are knocked offline because of an incident in the first two weeks after our product launches.

    ---
    template: inverse
    layout: true
    ---

    ## Let's measure the likelihood of this scenario occurring.

    ---

    ### We will do this with:
    ## A forecast.
    ## üå©Ô∏è

    ---

    ## 1. Will this occur this year?

    --

    ## 2. What are the odds that it will?

    ---

    ## Don't be over concerned with being correct.
    ### Try your best.

    --

    ### In fact, being "correct" might not matter.

    --

    ### We will cover that point later.

    ---

    ## Here's an example scenario.

    ---
    ## Scenario:
    ### We are knocked offline because of an incident in the next year.
    ---
    ## Your forecast:
    ### There is a .red[33% chance] that this will happen.

    ---

    ## Great! You've begun measurement.

    --

    ## Now go back to work!

    ---

    ### Just keep doing whatever it is you do as a _security professional_.

    ---

    ### When you're done with your work mitigating something, measure again.

    ---
    ## Scenario:
    ### We are knocked offline because of an incident in the next year.

    --

    ### Now consider your work: (_"And now we have implemented DDoS protection."_)
    ---
    ## Your forecast:
    ### There is a .red[29% chance] (.green[-4%] reduction) that this will happen.

    --

    ### (Remember, your last forecast was .red[33%]!)

    ---

    ### You're done. You have measured a .green[4%] reduction in the occurance of a risk.

    ---

    ### You might be asking yourself...

    --

    ## That was too easy.

    ---

    ### You would be right. This was an estimation without _rigor_.


    ---

    ## This forecast has lots of problems.

    ---

    ### But it is useful for self assessment.

    ---

    ### And it was extremely cheap and time effective.

    ---

    ## Now, let's make it more credible.

    --

    ### We can add several layers of rigor to this forecast.

    --

    ### This makes forecast metrics very useful and collaborative.

    ---
    template: final
    layout: false
    ## First: What is wrong?

    --

    ### People are highly biased with guesses. [Research](https://machinelearnings.co/why-are-we-so-confident-2c3151a6d5d0) agrees.

    --

    ### A [decade of research](https://www.amazon.com/Expert-Political-Judgment-Good-Know/dp/0691128715) shows experts to be hilariously weak at predictions.

    --

    ### Risk involves the probability of future events.

    --

    ### As "experts", we tend to be .red[worse] at predicting risks.

    --

    ### This fact is based on nobel prize winning research.

    --

    ### These studies are repeated with [humorous results](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow) in following research.

    ---
    template: inverse
    layout: true
    ---

    ### This may be one of the reasons we are so bad at preventing breaches.

    ---

    ### By default, we are weak at predictions, and worse at measuring risk.

    ---

    ## This issue is rampant in our industry.

    ---

    ### But there's hope!

    ---

    ## High risk industries rely on probabilistic approaches to risk.

    --

    ### [Space](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20120001369.pdf), [Nuclear](https://www.nrc.gov/about-nrc/regulatory/risk-informed/pra.html), [Oil](https://www.bsee.gov/sites/bsee.gov/files/interagency-agreements-mous-moas//nasa-bsee-iaa-1-28-16.pdf), [Environmental](https://www.epa.gov/risk/policy-use-probabilistic-analysis-risk-assessment-epa)

    --

    ## For some reason, we haven't been.

    ---

    ## Forecasting fills in the gaps where "_We don't have data!_"

    --

    ### It turns out, this is not a valid excuse in any other industry.

    ---

    ## It's easy to mimic the habits of [effective forecasters](https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock-ebook/dp/B00RKO6MS8).

    --

    ### They exist! They have been studied vigorously. For decades.

    ---

    ## These methods are easy to copy, with measurable results.

    ---

    ### This means rigorous forecasts can be relied on for decisions.

    --

    ### That .green[4%] value could be a reliable piece of data...

    --

    ### ...instead of feeling like a guess.

    ---

    # How do we fix our forecast?

    ---

    ### The forecast needs _discipline_ to increase the value of the _forecaster_.

    ---

    ### Let's build that process.

    ---

    ## First, let's pick good scenarios.

    ---

    # An employee is infected with malware.
    ### üíªÔ∏è

    ---

    ## First:
    ### It's not specific enough to forecast. Let's time box it to "this year".

    ---

    ## _An employee is infected with malware..._

    --

    # This quarter.

    ---

    ### Add specific threats, vector, asset, as makes sense.

    ---

    ## _An employee is infected with malware this year_

    --

    ### How about...

    ---

    ## A _C-Level executive_ is infected with malware this year.

    ---

    ### This could be easier to actually measure.

    ---
    ### Some examples:

    ---
    ## A C-Level executive is infected with malware in the next quarter.
    ---
    ## An employee is infected by an email attachment, resulting in a .red[SEV1] incident this year.
    ---
    ## Compromised employees are discovered by malware discovered as a result of next week's C&C hunt.
    ---

    ### These are better, but they don't have to be perfect.

    --

    ### Picking a good scenario is a lot like agreeing on a problem.

    --

    ## Nearly any future event can be forecasted.

    ---

    ### So long as it is concrete.

    ---

    ## Second: Gather supporting data.

    ### Good scenarios can [break down](https://en.wikipedia.org/wiki/Fermi_problem) into data that resembles our problem.

    --

    ### Previous metrics, similar forcasts, historical data, industry data, etc.

    ---

    ## A forecast improves with any "nearby" data.
    --

    ###  Even just a _little bit_ helps. You might not have any. That's OK too.

    ---

    ## Third: Train you, the forecaster.

    --

    ### We calibrate the forecaster's sense of certainty.

    --

    ### You're going to get better at representing your certainty *as a number*.

    ---

    ### This fixes problems with forecasting bias.

    ---

    ### It helps us avoid overconfidence.
    ### It sharpens our personal "certainty" about information.

    --

    ## It becomes simple for us to assign numbers to our certainty.
    ## (From 0-100%)

    ---
    ### You can quickly improve your own skills [here](http://confidence.success-equation.com), or [here](http://sethrylan.org/bayesian/calibrate.html).
    .footnote[A paid version is [here](https://good-judgment.thinkific.com/courses/Superforecasting-Fundamentals)]
    ---

    ## That training isn't really that hard.
    ### [Recent studies](https://hbr.org/2016/05/superforecasting-how-to-upgrade-your-companys-judgment) show that minimal training will drastically improve forecasting abilities.
    ---
    ## That means you just practiced a little.
    ---
    ## That also means your forecasts are now more reliable.
    ---
    ## We can improve this even *more*.
    ---
    ## Fourth: We introduce a panel of forecasters.
    ---
    ## Diverse panels of forecasters reduce bias from a forecast.
    ### You'd grab some friends, co-workers, experts, etc. Diverse is better.
    ---
    ## Then, we'd calibrate them, too.
    ###<i class="fa fa-user" aria-hidden="true"></i> <i class="fa fa-user" aria-hidden="true"></i> <i class="fa fa-user" aria-hidden="true"></i> <i class="fa fa-user" aria-hidden="true"></i> <i class="fa fa-user" aria-hidden="true"></i>

    --

    ### Now we can average them together into one forecast:

    --

    ### <i class="fa fa-user" aria-hidden="true"></i>

    --

    ### And this number represents a consensus estimate.

    ---
    ### Groups seem to benefit greatly from combined forecasts.
    ---
    ## A fun example.

    --

    ### In 1907, a group of 787 people [correctly guessed the weight of an Ox](https://arxiv.org/pdf/1410.3989.pdf).

    --

    .footnote[see [The Wisdom of Crowds](https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds)]


    ---

    ## A recent example.

    --

    ### This method was used to create [teams of civilians](https://www.npr.org/sections/parallels/2014/04/02/297839429/-so-you-think-youre-smarter-than-a-cia-agent)...

    --

    ### ...who .red[beat] national security intelligence analysts...

    --

    ###...who had access to .red[confidential data].

    ---

    ### So, where are we?

    --

    ## Lets summarize.

    ---

    name: final
    layout: true
    class: left, top, inverse

    ---

    # We have forecasting tools.

    --

    ### Forecasts are improved with specific, time boxed scenarios.

    --

    ### Groups of diverse expert forecasts can smoothen out bias, like FUD and overconfidence.

    --

    ### We can be trained to calibrate ourselves. [Quickly](http://journal.sjdm.org/16/16511/jdm16511.pdf).

    --

    ### So...

    --

    ## We can create high value, rigorous forecasts about the probability of a risk.

    ---
    template: inverse
    layout: true
    ---

    ## We're still not done.

    --

    ### This is *still not useful*.

    --

    ### We haven't measured that what you are working on _is valuable_

    --

    ## We'll get there!

    ---

    ### To understand how we measure risk, we first need to acknowledge how little we know about risk.

    ---

    ### Who says we can always predict the future?

    ---

    ### We can't, really.

    ---

    ### We can't know what future an Oracle holds for us.



    ### üååüßô‚Äç‚ôÇÔ∏è‚Äçüìñüßô‚Äç‚ôÄÔ∏èüåå

    ---

    ## There is an _actual_ probibility we don't see.

    ---

    ## Some events we need to forecast cannot be tracked afterwards.

    ---

    ### There are _oracles_ we don't get to see.

    ---

    ## We have to work without access to a probability that only _omniscience_ can reveal.

    ---

    ## With these methods,
    ## we're not pretending that we can predict the future.

    ---

    ### We're not trying to hit a bullseye an Oracle keeps changing on us.

    --

    ### We're just trying to get measurably closer to one.

    ---

    ## This is a matter of accuracy versus *precision*.

    --

    ### Accuracy needs to be close to the true probability...

    --

    ### ...so we often can't prove that we are _accurately_ forecasting a risk.

    ---

    ## However!

    ---

    ### Precision is about reliably creating the same result, even if we're off-center.

    ---

    ### To put this simply...

    --

    ## We may never find a "bullseye".

    --

    ## Instead, we will measure progress towards it.

    --

    ## üéØ

    ---

    ## Let's measure your work.
    ### Assume you are improving the security of your AWS infrastructure.

    --

    ### Let's run forecasts every quarter while we mitigate this risk.

    ---
    template: final
    layout: true
    ---

    ##Scenario: _"An attacker can access destructive AWS IAM permissions in the next 365 days."_

    --
    #### 2016 - Q3: .red[25%]
    First forecast. We haven't fixed anything yet.
    --

    #### 2016 - Q4: .red[23%]
    We have limited the destructive capability of keys in production.
    --

    #### 2017 - Q1: .red[16%]
    We added multifactor protection to keys used by engineers.
    --

    #### 2017 - Q2: .green[10%]
    We took keys out of source code and use roles now.
    --

    ## An increase in confidence against this risk of .green[15%].

    ---

    ## This directionality of confidence is all that matters.

    --

    ### It is impossible to read the oracle of probability for each risk.

    --

    ### If our efforts mitigate risk, we believe we influence these "oracles".

    --

    ### Then our forecasts of risk likely share a direction with an unknown oracle.

    --

    ## If there is truth to this, then we have a huge opportunity...

    ---
    template: inverse

    ## We can measure that we're getting better at security.
    .footnote[Read more:  [1](https://medium.com/starting-up-security/killing-chicken-little-measure-and-eliminate-risk-through-forecasting-ecdf4c7e9575), [2](https://magoo.github.io/simple-risk-analysis/).]

    </textarea>
        <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    <script>
      var hljs = remark.highlighter.engine;
    </script>
    <script src="remark.language.js"></script>
    <script>
      var slideshow = remark.create({
          highlightStyle: 'monokai',
          highlightLanguage: 'remark',
          highlightLines: true
        }) ;
    </script>
    <script>
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-44561333-1']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script');
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.scripts[0];
        s.parentNode.insertBefore(ga, s);
      }());
    </script>
  </body>
</html>
